{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "os is a library that allows us to use \"operating system dependant functionalities.\" Here, we can use the .listdir() method to list the contents of a directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets try and pull data from the files in this folder\n",
      "Huzzah! Mission Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwama\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2993, 104)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_dfs = []\n",
    "\n",
    "try: \n",
    "    print('Lets try and pull data from the files in this folder')\n",
    "    for file in os.listdir('../data')[1:]:\n",
    "        d = pd.read_csv('../data/' + file)\n",
    "        list_of_dfs.append(d)\n",
    "    print('Huzzah! Mission Complete')\n",
    "except:\n",
    "    print(\"Welp, that didn't work\")\n",
    "\n",
    "df = pd.concat(list_of_dfs, ignore_index=True).drop_duplicates(subset = 'title')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we, perhaps, have hundreds of csvs in the data subfolder, we don't want to manually create hundreds of dataframes just to concatenate them. Instead, we can programatically access a list of what's contained inside the data subfolder, import, store, and then concatenate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['all_awardings', 'allow_live_comments', 'approved_at_utc',\n",
       "       'approved_by', 'archived', 'author', 'author_cakeday',\n",
       "       'author_flair_background_color', 'author_flair_css_class',\n",
       "       'author_flair_richtext',\n",
       "       ...\n",
       "       'thumbnail_width', 'title', 'total_awards_received', 'ups', 'url',\n",
       "       'user_reports', 'view_count', 'visited', 'whitelist_status', 'wls'],\n",
       "      dtype='object', length=104)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>...</th>\n",
       "      <th>thumbnail_width</th>\n",
       "      <th>title</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>view_count</th>\n",
       "      <th>visited</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>pittman66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAL</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wiki Overhaul Month, Week 2: Watch Order Wiki</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>https://www.reddit.com/r/anime/comments/c822ra...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>AnimeMod</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recommendation Tuesdays Megathread - Week of J...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>https://www.reddit.com/r/anime/comments/c823rj...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>MinecrafterPH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#2e51a2</td>\n",
       "      <td>MAL</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My Hero Academia Season 4 is reportedly listed...</td>\n",
       "      <td>0</td>\n",
       "      <td>5778</td>\n",
       "      <td>https://www.reddit.com/r/anime/comments/c8o432...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments  approved_at_utc  approved_by  archived  \\\n",
       "0            []                False              NaN          NaN     False   \n",
       "1            []                False              NaN          NaN     False   \n",
       "2            []                 True              NaN          NaN     False   \n",
       "\n",
       "          author author_cakeday author_flair_background_color  \\\n",
       "0      pittman66            NaN                           NaN   \n",
       "1       AnimeMod            NaN                           NaN   \n",
       "2  MinecrafterPH            NaN                       #2e51a2   \n",
       "\n",
       "  author_flair_css_class author_flair_richtext  ... thumbnail_width  \\\n",
       "0                    MAL                    []  ...             NaN   \n",
       "1                    NaN                    []  ...             NaN   \n",
       "2                    MAL                    []  ...             NaN   \n",
       "\n",
       "                                               title total_awards_received  \\\n",
       "0      Wiki Overhaul Month, Week 2: Watch Order Wiki                     0   \n",
       "1  Recommendation Tuesdays Megathread - Week of J...                     0   \n",
       "2  My Hero Academia Season 4 is reportedly listed...                     0   \n",
       "\n",
       "    ups                                                url user_reports  \\\n",
       "0    67  https://www.reddit.com/r/anime/comments/c822ra...           []   \n",
       "1    61  https://www.reddit.com/r/anime/comments/c823rj...           []   \n",
       "2  5778  https://www.reddit.com/r/anime/comments/c8o432...           []   \n",
       "\n",
       "   view_count  visited  whitelist_status  wls  \n",
       "0         NaN    False           all_ads    6  \n",
       "1         NaN    False           all_ads    6  \n",
       "2         NaN    False           all_ads    6  \n",
       "\n",
       "[3 rows x 104 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anime     2155\n",
       "KDRAMA     838\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The unbalanced classes may pose a problem for my model because it may struggle with assigning new data to the kdrama class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Wiki Overhaul Month, Week 2: Watch Order Wiki\n",
       "1    Recommendation Tuesdays Megathread - Week of J...\n",
       "2    My Hero Academia Season 4 is reportedly listed...\n",
       "3     Dumbbell Nan Kilo Moteru? - Episode 1 discussion\n",
       "4    Best Girl 6: Starting Salt in Another Contest!...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to focus on the subreddit post titles. I'll split my data using Train, Test, Split to see if my model can handle new data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['subreddit'].map(lambda cell: 1 if cell == 'anime' else 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "# Model Comparison using NLP, Pipelines, and GridSearch\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "This is the baseline for my other models. If they do not surpass this score, then they are not worth using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7200133645172068"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize = True).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logisitic Regression\n",
    "\n",
    "This is a classification model that excels on data with a binarized target variable and is used for its readability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Count Vectorizer*\n",
    "\n",
    "This is an NLP method that converts my title strings into column names and counts how many time those words appear in all of the titles I'm looking at. It's basically like `.value_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates pipeline for CountVectorizer and Logistic Regression\n",
    "pipe_cv_lr = Pipeline([('cv', CountVectorizer(stop_words = 'english', max_df = .9)),  \n",
    "                ('lr', LogisticRegression(random_state = 42))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * stop_words are words like 'the', 'an', 'to'. I don't want to include those as columns\n",
    "* max_df means the min percentage of documents something needs to be in order to be excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters for GridSearch to use to find best possible CountVectorizer, Log Regression hyperparameters\n",
    "\n",
    "pipe_cv_lr_params = {\n",
    "    'cv__max_features': [1500, 2000, 2500],   #max number of columns\n",
    "    \n",
    "    'cv__min_df': [3, 5],  #minimum number of docments something needs to be in in order to be included\n",
    "    \n",
    "    'cv__ngram_range': [(1,1), (1,2)],   #Accounts for context of up to two words i.e  'not good' vs 'not' or 'good'\n",
    "    \n",
    "    'lr__C': [.5, 1]  #penalty on coefficients increases as C decreases\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwama\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CountVectorizer, Logistic Regression train score is 0.9295900178253119\n",
      "The CountVectorizer, Logistic Regression test score is 0.9252336448598131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cv__max_features': 1500,\n",
       " 'cv__min_df': 3,\n",
       " 'cv__ngram_range': (1, 2),\n",
       " 'lr__C': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearching to find best combination of hyperparameters for CountVectorizer\n",
    "gs_cv_lr = GridSearchCV(pipe_cv_lr, param_grid=pipe_cv_lr_params, cv = 3)\n",
    "gs_cv_lr.fit(X_train, y_train)\n",
    "print(f'The CountVectorizer, Logistic Regression train score is {gs_cv_lr.best_score_}')\n",
    "print(f'The CountVectorizer, Logistic Regression test score is {gs_cv_lr.score(X_test, y_test)}')\n",
    "gs_cv_lr.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a good understanding of how our model will do when given new data, I'll generate a confusion matrix on my testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Negative</th>\n",
       "      <th>Actual Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicted Negative</th>\n",
       "      <td>156</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted Positive</th>\n",
       "      <td>2</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Actual Negative  Actual Positive\n",
       "Predicted Negative              156               54\n",
       "Predicted Positive                2              537"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_true=y_test,   #Actuals\n",
    "                                            y_pred=gs_cv_lr.predict(X_test)),   #Generate Predictions\n",
    "             columns=['Actual Negative', 'Actual Positive'],\n",
    "             index=['Predicted Negative','Predicted Positive'])\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix lets us know how well our model is adapting to new data. The positive or majority class is Anime and the negative or minority class is KDrama. Because I stratified my data before I split it, the inequality in the number of true positive (*bottom right*) and true negative (*top left*) values lets you know that there is an imbalance in the amount of Anime and KDrama posts in my data. \n",
    "> * I have signigicantly more true positives and negatives than false positives (*bottom left*) and false negatives (*top right*), meaning that this model does a great job of correctly assigning new posts to the correct subreddit.\n",
    "* I have a good number of false negatives though, which lets me know that we are often assigning something to the KDrama class when it actually belongs to the Anime class.\n",
    "* The low number of false positives means that the model rarely assigns something to the KDrama class when it is really an anime. \n",
    "\n",
    "The last two cases are probably due to the fact that our data is imbalanced, causing our model to be less prepared to handle new KDrama data because it needs to learn a bit more about it before it can accurately be sorted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *TFIDF Vectorizer*\n",
    "\n",
    "Similar to Count Vectorizer, this NLP method converts my titles into strings. Unlike CountVectorizer, TFIDFVectorizer assigns a float score to each of the words in the title based on how often they appear in all of my documents.\n",
    "> * words that appear more often in one document but rarely in the rest of them will score higher (i.e names)\n",
    "* words that appear often in one document and show up in every document will score lower(i.e the)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates pipeline for TFIDFVectorizer and Logistic Regression\n",
    "pipe_tf_lr = Pipeline([('tf', TfidfVectorizer(stop_words = 'english', max_df = .9)),\n",
    "                ('lr', LogisticRegression(random_state = 42))])\n",
    "\n",
    "#Hyperparameters for GridSearch to use to find best possible tfVectorizer, Log Regression hyperparameters\n",
    "\n",
    "pipe_tf_lr_params = {\n",
    "    'tf__max_features': [3000, 3500, 4000],\n",
    "    'tf__min_df': [3, 5],\n",
    "    'tf__ngram_range': [(1,1), (1,2)],  \n",
    "    'lr__C': [.5, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwama\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TFIDFVectorizer, Logistic Regression train score is 0.9459617635583301\n",
      "The TFIDFVectorizer, Logistic Regression test score is 0.9566998244587478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lr__C': 1,\n",
       " 'tf__max_features': 3000,\n",
       " 'tf__min_df': 3,\n",
       " 'tf__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearching to find best combination of hyperparameters for tfVectorizer\n",
    "gs_tf_lr = GridSearchCV(pipe_tf_lr, param_grid=pipe_tf_lr_params, cv = 3)\n",
    "gs_tf_lr.fit(X_train, y_train)\n",
    "print(f'The TFIDFVectorizer, Logistic Regression train score is {gs_tf_lr.best_score_}')\n",
    "print(f'The TFIDFVectorizer, Logistic Regression test score is {gs_tf_lr.score(X_test, y_test)}')\n",
    "gs_tf_lr.best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_true=y_test,   #Actuals\n",
    "                                            y_pred=gs_tf_lr.predict(X_test)),   #Generate Predictions\n",
    "             columns=['Actual Negative', 'Actual Positive'],\n",
    "             index=['Predicted Negative','Predicted Positive'])\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It turns out that both Logistic Regression Models perform really strongly and adapt to new data really well.** The Count Vectorizer model is pulling ahead of the TFIDF model, but this could be changed by modifying the gridsearch hyperparameters. \n",
    "> Due to it's performance, I will be comparing the **Logistic Regression Count Vectorizer model (LRCV)** to the remaining models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Multinomial Naive Bayes\n",
    "\n",
    "This is a modeling technique that relies on Bayes Theorem to make classification. It also models with the assumption that all of our features are independent of one another, which is rarely met. Although that assumption is naive, this model performs amazingly well regardless of that fact. We will use the Multinomial Version because our column values are positive integers after CountVectorizering our data.\n",
    "\n",
    "> This model should give us a much better train and test score than the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates pipeline for TFIDFVectorizer and Gaussian Naive Bayes\n",
    "pipe_mnb = Pipeline([('cv', CountVectorizer(stop_words = 'english', max_df = .9)), \n",
    "                ('mnb', MultinomialNB())])\n",
    "\n",
    "#Hyperparameters for GridSearch to use to find best possible tfVec, Gaussian Naive Bayes hyperparameters\n",
    "\n",
    "pipe_mnb_params = {\n",
    "    'cv__max_features': [1500, 2000, 2500],\n",
    "    'cv__min_df': [3, 5],\n",
    "    'cv__ngram_range': [(1,1), (1,2)]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CountVectorizer, Multnomial Naive Bayes train score is 0.9541552867733125\n",
      "The CountVectorizer, Multinomial Naive Bayes test score is 0.9561146869514335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cv__max_features': 2500, 'cv__min_df': 3, 'cv__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearching to find best combination of hyperparameters for tfVectorizer\n",
    "gs_mnb = GridSearchCV(pipe_mnb, param_grid=pipe_mnb_params, cv = 3)\n",
    "gs_mnb.fit(X_train, y_train)\n",
    "print(f'The CountVectorizer, Multnomial Naive Bayes train score is {gs_mnb.best_score_}')\n",
    "print(f'The CountVectorizer, Multinomial Naive Bayes test score is {gs_mnb.score(X_test, y_test)}')\n",
    "gs_mnb.best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_true=y_test,   #Actuals\n",
    "                                            y_pred=gs_mnb.predict(X_test)),   #Generate Predictions\n",
    "             columns=['Actual Negative', 'Actual Positive'],\n",
    "             index=['Predicted Negative','Predicted Positive'])\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Count Vectorized Multinomial Bayes model performed very strongly with an extremely high train score. It also sports high adaptability to new data with the test score being higher than the train score. However, our model is less accurate and a weaker performer when adapting to new data when compared to the LRCV model. This may be because the naive assumption that's trademark to the Naive Bayes model is causing some error. It may also be because of the hyperparameters currently set. \n",
    "\n",
    "*As we continue, one major factor in the performance of our models will be the hyperparameters that each model is tested on. The nature of grid searching is a guess and check, so a good estimate can lead to amazing results for one model, and poor results for another.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Gaussian Naive Bayes\n",
    "\n",
    "This is a another modeling technique that relies on Bayes Theorem to make classification. It also models with the assumption that all of our features are independent of one another, which is rarely met. Although that assumption is naive, this model performs amazingly well regardless of that fact. We will use the Gaussian Version because this version is the only one compatible with TFIDF's float scoring.\n",
    "\n",
    "> This model should give us a much better train and test score than the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a class that we will add to our pipeline to allow our Gaussian Naive Bayes(GNB) model to work. Without it, our data will not transform from a sparse matrix to a dense matrix, which is required by GNB.\n",
    "\n",
    "**Code adapted from StackOverflow**\n",
    "> https://stackoverflow.com/questions/28384680/scikit-learns-pipeline-a-sparse-matrix-was-passed-but-dense-data-is-required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates pipeline for TFIDFVectorizer and Gaussian Naive Bayes\n",
    "pipe_gnb = Pipeline([('tf', TfidfVectorizer(stop_words = 'english', max_df = .9)),\n",
    "                ('to_dense', DenseTransformer()), \n",
    "                ('gnb', GaussianNB())])\n",
    "\n",
    "#Hyperparameters for GridSearch to use to find best possible tfVec, Gaussian Naive Bayes hyperparameters\n",
    "\n",
    "pipe_gnb_params = {\n",
    "    'tf__max_features': [3500, 4000, 4500],\n",
    "    'tf__min_df': [3, 5],\n",
    "    'tf__ngram_range': [(1,1), (1,2)]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TFIDFVectorizer, Gaussian Naive Bayes train score is 0.908115489660554\n",
      "The TFIDFVectorizer, Gaussian Naive Bayes test score is 0.9093036863662961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tf__max_features': 3500, 'tf__min_df': 3, 'tf__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearching to find best combination of hyperparameters for tfVectorizer\n",
    "gs_gnb = GridSearchCV(pipe_gnb, param_grid=pipe_gnb_params, cv = 3)\n",
    "gs_gnb.fit(X_train, y_train)\n",
    "print(f'The TFIDFVectorizer, Gaussian Naive Bayes train score is {gs_gnb.best_score_}')\n",
    "print(f'The TFIDFVectorizer, Gaussian Naive Bayes test score is {gs_gnb.score(X_test, y_test)}')\n",
    "gs_gnb.best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_true=y_test,   #Actuals\n",
    "                                            y_pred=gs_gnb.predict(X_test)),   #Generate Predictions\n",
    "             columns=['Actual Negative', 'Actual Positive'],\n",
    "             index=['Predicted Negative','Predicted Positive'])\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Gaussian Naive Bayes Model didn't come close to the efficiency of the LRCV model. This is not a huge surprise because the TFIDF model performed worse than the CountVectorized model, so a model based on it should be less accurate. This is still a very strong model that adapts to new data well and is not overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Decision Tree\n",
    "\n",
    "Decision trees work by attempting to reduce a Gini score to 0. A gini score is basically how pure a group of sample data is. A model with a gini score of 0 has final samples that consist of the same thing (i.e. all red birds). Decision trees want to get 1 sample per final sample group, which is what leads to overfitting. With this knowledge in mind, I'll use gridsearching to find the optimal hyperparamters to alleviate that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Count Vectorizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_dt_cv = Pipeline([('cv', CountVectorizer(stop_words = 'english', max_df = .9)),\n",
    "                ('dt', DecisionTreeClassifier(max_depth = 5, random_state = 42))])\n",
    "\n",
    "#max_depth is the number of levels/questions asked by the decision tree\n",
    "\n",
    "pipe_dt_cv_params = {\n",
    "    'cv__max_features': [1500, 2000, 2500],\n",
    "    'cv__min_df': [3, 5],\n",
    "    'cv__ngram_range': [(1,1), (1,2)],\n",
    "    'dt__min_samples_leaf': [3, 5],   #Minimum number of samples required before splitting a group/node\n",
    "    'dt__min_samples_split': [7, 10]   #Minimum number of samples required to be in a group/node\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CountVectorizer, Decision Tree train score is 0.8312524385485759\n",
      "The CountVectorizer, Decision Tree test score is 0.8203627852545348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cv__max_features': 1500,\n",
       " 'cv__min_df': 3,\n",
       " 'cv__ngram_range': (1, 1),\n",
       " 'dt__min_samples_leaf': 3,\n",
       " 'dt__min_samples_split': 7}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearching to find best combination of hyperparameters for cvVectorizer\n",
    "gs_dt_cv = GridSearchCV(pipe_dt_cv, param_grid=pipe_dt_cv_params, cv = 3)\n",
    "gs_dt_cv.fit(X_train, y_train)\n",
    "print(f'The CountVectorizer, Decision Tree train score is {gs_dt_cv.best_score_}')\n",
    "print(f'The CountVectorizer, Decision Tree test score is {gs_dt_cv.score(X_test, y_test)}')\n",
    "gs_dt_cv.best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_true=y_test,   #Actuals\n",
    "                                            y_pred=gs_dt_cv.predict(X_test)),   #Generate Predictions\n",
    "             columns=['Actual Negative', 'Actual Positive'],\n",
    "             index=['Predicted Negative','Predicted Positive'])\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *TFIDF Vectorizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_dt_tf = Pipeline([('tf', TfidfVectorizer(stop_words = 'english', max_df = .9)),\n",
    "                ('dt', DecisionTreeClassifier(max_depth = 5, random_state = 42))])\n",
    "\n",
    "#max_depth is the number of levels/questions asked by the decision tree\n",
    "\n",
    "pipe_dt_tf_params = {\n",
    "    'tf__max_features': [3500, 4000, 4500],\n",
    "    'tf__min_df': [3, 5],\n",
    "    'tf__ngram_range': [(1,1), (1,2)],\n",
    "    'dt__min_samples_leaf': [3, 5],   #Minimum number of samples required before splitting a group/node\n",
    "    'dt__min_samples_split': [7, 10]   #Minimum number of samples required to be in a group/node\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TFIDFVectorizer, Decision Tree train score is 0.831642606320718\n",
      "The TFIDFVectorizer, Decision Tree test score is 0.8227033352837917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dt__min_samples_leaf': 3,\n",
       " 'dt__min_samples_split': 7,\n",
       " 'tf__max_features': 3500,\n",
       " 'tf__min_df': 3,\n",
       " 'tf__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearching to find best combination of hyperparameters for tfVectorizer\n",
    "gs_dt_tf = GridSearchCV(pipe_dt_tf, param_grid=pipe_dt_tf_params, cv = 3)\n",
    "gs_dt_tf.fit(X_train, y_train)\n",
    "print(f'The TFIDFVectorizer, Decision Tree train score is {gs_dt_tf.best_score_}')\n",
    "print(f'The TFIDFVectorizer, Decision Tree test score is {gs_dt_tf.score(X_test, y_test)}')\n",
    "gs_dt_tf.best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_true=y_test,   #Actuals\n",
    "                                            y_pred=gs_dt_tf.predict(X_test)),   #Generate Predictions\n",
    "             columns=['Actual Negative', 'Actual Positive'],\n",
    "             index=['Predicted Negative','Predicted Positive'])\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that although we added base parameters to lower our error due to variance significantly, the decision tree model was a weak performer. **Due to the innately high risk of overfitting with this model and because our data is already unbalanced, this is not the ideal model for our dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Random Forest \n",
    "\n",
    "Random Forests are aggregated decision trees that use a concept called bagging to make them very accurate. Bagging is basically the central limit theorem; we split a population (our data) into multiple samples and run decision tree models over each sample. Then we take the average of each model. This is an ensemble model, and these models *typically* have the highest accuracy scores for three reasons. \n",
    "> * First, ensemble models take the average of multiple models, which usually results in canceling out the error in each of the individuals models. \n",
    "* Second, taking the average scores of multiple models tends to result in reaching scores one model may not have been able to reach alone. This means we may be able to hone in on a global best. \n",
    "* Finally, one model will most likely be not perfect since they all have their shortcomings. Aggregating the results of multiple models could create a model that exceeds the limitations of all the components combined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Count Vectorizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf_cv = Pipeline([('cv', CountVectorizer(stop_words = 'english', max_df = .9)),\n",
    "                ('rf', RandomForestClassifier(max_depth = 5, random_state = 42))])\n",
    "\n",
    "pipe_rf_cv_params = {\n",
    "    'cv__max_features': [2500, 3500, 4000, 4500],\n",
    "    'cv__min_df': [3, 5],\n",
    "    'cv__ngram_range': [(1,1), (1,2)],\n",
    "    'rf__n_estimators': [50 ,75],\n",
    "    'rf__max_depth': [5, 6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CountVectorizer, Random Forest train score is 0.7631681623097932\n",
      "The CountVectorizer, Random Forest test score is 0.7495611468695144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cv__max_features': 2500,\n",
       " 'cv__min_df': 5,\n",
       " 'cv__ngram_range': (1, 1),\n",
       " 'rf__max_depth': 6,\n",
       " 'rf__n_estimators': 75}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearching to find best combination of hyperparameters for tfVectorizer\n",
    "gs_rf_cv = GridSearchCV(pipe_rf_cv, param_grid=pipe_rf_cv_params, cv = 3)\n",
    "gs_rf_cv.fit(X_train, y_train)\n",
    "print(f'The CountVectorizer, Random Forest train score is {gs_rf_cv.best_score_}')\n",
    "print(f'The CountVectorizer, Random Forest test score is {gs_rf_cv.score(X_test, y_test)}')\n",
    "gs_rf_cv.best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_true=y_test,   #Actuals\n",
    "                                            y_pred=gs_rf_cv.predict(X_test)),   #Generate Predictions\n",
    "             columns=['Actual Negative', 'Actual Positive'],\n",
    "             index=['Predicted Negative','Predicted Positive'])\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *TFIDF Vectorizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf_tf = Pipeline([('tf', TfidfVectorizer(stop_words = 'english', max_df = .9)),\n",
    "                ('rf', RandomForestClassifier(max_depth = 5, random_state = 42))])\n",
    "\n",
    "pipe_rf_tf_params = {\n",
    "    'tf__max_features': [2500, 3500, 4000, 4500],\n",
    "    'tf__min_df': [3, 5],\n",
    "    'tf__ngram_range': [(1,1), (1,2)],\n",
    "    'rf__n_estimators': [100, 125],\n",
    "    'rf__max_depth': [5, 6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TFIDFVectorizer, Random Forest train score is 0.7641435817401483\n",
      "The TFIDFVectorizer, Random Forest test score is 0.7466354593329433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rf__max_depth': 6,\n",
       " 'rf__n_estimators': 75,\n",
       " 'tf__max_features': 2500,\n",
       " 'tf__min_df': 5,\n",
       " 'tf__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearching to find best combination of hyperparameters for tfVectorizer\n",
    "gs_rf_tf = GridSearchCV(pipe_rf_tf, param_grid=pipe_rf_tf_params, cv = 3)\n",
    "gs_rf_tf.fit(X_train, y_train)\n",
    "print(f'The TFIDFVectorizer, Random Forest train score is {gs_rf_tf.best_score_}')\n",
    "print(f'The TFIDFVectorizer, Random Forest test score is {gs_rf_tf.score(X_test, y_test)}')\n",
    "gs_rf_tf.best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_true=y_test,   #Actuals\n",
    "                                            y_pred=gs_rf_tf.predict(X_test)),   #Generate Predictions\n",
    "             columns=['Actual Negative', 'Actual Positive'],\n",
    "             index=['Predicted Negative','Predicted Positive'])\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shockingly, the ensemble model yielded the worst results of all. This is without a doubt due to the hyperparameters we have set. In all scenarios, the random forest classifier should outperform the decision trees model. Because we are using a gridsearch and random forests have 2 more important hyperparameters in addition to those present for decision trees, I have to decide how much computer power and time I want to give to this model. This is definitely a learning experience, and shows how the flaw with gridsearching can lead to very interesting results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model accuracy score is 0.7252377468910022.\n",
      "The CountVectorizer, Logistic Regression train score is 0.9576667967225907\n",
      "The TFIDFVectorizer, Logistic Regression train score is 0.9459617635583301\n",
      "The CountVectorizer, Multnomial Naive Bayes train score is 0.9541552867733125\n",
      "The TFIDFVectorizer, Gaussian Naive Bayes train score is 0.908115489660554\n",
      "The CountVectorizer, Decision Tree train score is 0.8312524385485759\n",
      "The TFIDFVectorizer, Decision Tree train score is 0.831642606320718\n",
      "The CountVectorizer, Random Forest train score is 0.7631681623097932\n",
      "The TFIDFVectorizer, Random Forest train score is 0.7641435817401483\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score\n",
    "print(f'Baseline model accuracy score is {y.value_counts(normalize = True).max()}.')\n",
    "print(f'The CountVectorizer, Logistic Regression train score is {gs_cv_lr.best_score_}')\n",
    "print(f'The TFIDFVectorizer, Logistic Regression train score is {gs_tf_lr.best_score_}')\n",
    "print(f'The CountVectorizer, Multnomial Naive Bayes train score is {gs_mnb.best_score_}')\n",
    "print(f'The TFIDFVectorizer, Gaussian Naive Bayes train score is {gs_gnb.best_score_}')\n",
    "print(f'The CountVectorizer, Decision Tree train score is {gs_dt_cv.best_score_}')\n",
    "print(f'The TFIDFVectorizer, Decision Tree train score is {gs_dt_tf.best_score_}')\n",
    "print(f'The CountVectorizer, Random Forest train score is {gs_rf_cv.best_score_}')\n",
    "print(f'The TFIDFVectorizer, Random Forest train score is {gs_rf_tf.best_score_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Although all of our models performed better than the baseline, I would go with the CountVectorized Logisitic Regression model to sort between the Anime and KDrama subreddit. It sported the best train and test scores while not being overfit. In the future, I could definitely increase the accuracy of my models by tweaking the hyperparameters each model gridsearches over. Our worst performing models were decision trees and random forests, which is probably due to our dataset imbalance. Our strongest performing models were the Logisitic Regression models, which is without a doubt due to the fact that we have an easily binarized target variable. The Naive Bayes models also performed really strongly and would be solid alternatives to the Logistic Regression models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
