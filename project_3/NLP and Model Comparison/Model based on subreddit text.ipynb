{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets try and pull data from the files in this folder\n",
      "Huzzah! Mission Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwama\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2135, 104)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_dfs = []\n",
    "\n",
    "try: \n",
    "    print('Lets try and pull data from the files in this folder')\n",
    "    for file in os.listdir('../data')[1:]:\n",
    "        d = pd.read_csv('../data/' + file)\n",
    "        list_of_dfs.append(d)\n",
    "    print('Huzzah! Mission Complete')\n",
    "except:\n",
    "    print(\"Welp, that didn't work\")\n",
    "\n",
    "df = pd.concat(list_of_dfs, ignore_index=True).drop_duplicates(subset = 'selftext')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['all_awardings', 'allow_live_comments', 'approved_at_utc',\n",
       "       'approved_by', 'archived', 'author', 'author_cakeday',\n",
       "       'author_flair_background_color', 'author_flair_css_class',\n",
       "       'author_flair_richtext',\n",
       "       ...\n",
       "       'thumbnail_width', 'title', 'total_awards_received', 'ups', 'url',\n",
       "       'user_reports', 'view_count', 'visited', 'whitelist_status', 'wls'],\n",
       "      dtype='object', length=104)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>...</th>\n",
       "      <th>thumbnail_width</th>\n",
       "      <th>title</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>view_count</th>\n",
       "      <th>visited</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>pittman66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MAL</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wiki Overhaul Month, Week 2: Watch Order Wiki</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>https://www.reddit.com/r/anime/comments/c822ra...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>AnimeMod</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recommendation Tuesdays Megathread - Week of J...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>https://www.reddit.com/r/anime/comments/c823rj...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>MinecrafterPH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#2e51a2</td>\n",
       "      <td>MAL</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My Hero Academia Season 4 is reportedly listed...</td>\n",
       "      <td>0</td>\n",
       "      <td>5778</td>\n",
       "      <td>https://www.reddit.com/r/anime/comments/c8o432...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments  approved_at_utc  approved_by  archived  \\\n",
       "0            []                False              NaN          NaN     False   \n",
       "1            []                False              NaN          NaN     False   \n",
       "2            []                 True              NaN          NaN     False   \n",
       "\n",
       "          author author_cakeday author_flair_background_color  \\\n",
       "0      pittman66            NaN                           NaN   \n",
       "1       AnimeMod            NaN                           NaN   \n",
       "2  MinecrafterPH            NaN                       #2e51a2   \n",
       "\n",
       "  author_flair_css_class author_flair_richtext  ... thumbnail_width  \\\n",
       "0                    MAL                    []  ...             NaN   \n",
       "1                    NaN                    []  ...             NaN   \n",
       "2                    MAL                    []  ...             NaN   \n",
       "\n",
       "                                               title total_awards_received  \\\n",
       "0      Wiki Overhaul Month, Week 2: Watch Order Wiki                     0   \n",
       "1  Recommendation Tuesdays Megathread - Week of J...                     0   \n",
       "2  My Hero Academia Season 4 is reportedly listed...                     0   \n",
       "\n",
       "    ups                                                url user_reports  \\\n",
       "0    67  https://www.reddit.com/r/anime/comments/c822ra...           []   \n",
       "1    61  https://www.reddit.com/r/anime/comments/c823rj...           []   \n",
       "2  5778  https://www.reddit.com/r/anime/comments/c8o432...           []   \n",
       "\n",
       "   view_count  visited  whitelist_status  wls  \n",
       "0         NaN    False           all_ads    6  \n",
       "1         NaN    False           all_ads    6  \n",
       "2         NaN    False           all_ads    6  \n",
       "\n",
       "[3 rows x 104 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi everyone, welcome to the Wiki Overhaul Mont...</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nNeed a recommendation or have one to share? ...</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>According to [this post](https://i.imgur.com/b...</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*Dumbbell Nan Kilo Moteru?*, episode 1\\n\\nAlte...</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>###[Vote here](https://animebracket.com/vote/b...</td>\n",
       "      <td>anime</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            selftext subreddit\n",
       "0  Hi everyone, welcome to the Wiki Overhaul Mont...     anime\n",
       "1  \\nNeed a recommendation or have one to share? ...     anime\n",
       "2  According to [this post](https://i.imgur.com/b...     anime\n",
       "3  *Dumbbell Nan Kilo Moteru?*, episode 1\\n\\nAlte...     anime\n",
       "4  ###[Vote here](https://animebracket.com/vote/b...     anime"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['selftext', 'subreddit']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['selftext'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have 2152 Image/Video/Gif/Meme Subreddit posts. I'm going to drop them because I do not think they will be a good indicator of whether or not a post belongs to the anime or kdrama subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['selftext']\n",
    "y = df['subreddit'].map(lambda cell: 1 if cell == 'anime' else 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Model Comparison Using NLP, Pipelines, and GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline \n",
    "\n",
    "This is the baseline for my models. If a model does not at least do better than this score, then the model is not worth using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7582005623242737"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize = True).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "This is a model that excels on data with a binarized target variable. This model is also widely used for its simplicity and readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer\n",
    "\n",
    "This is an NLP method that converts my title strings into column names and counts how many time those words appear in all of the titles I'm looking at. It's basically like `.value_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates pipeline for CountVectorizer and Logistic Regression\n",
    "pipe_cv_lr = Pipeline([('cv', CountVectorizer(stop_words = 'english', max_df = .9)),  \n",
    "                ('lr', LogisticRegression(random_state = 42))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* stop_words are words like 'the', 'an', 'to'. I don't want to include those as columns\n",
    ">* max_df means the min percentage of documents something needs to be in order to be excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters for GridSearch to use to find best possible CountVectorizer, Log Regression hyperparameters\n",
    "\n",
    "pipe_cv_lr_params = {\n",
    "    'cv__max_features': [1500, 2000, 2500],   #max number of columns\n",
    "    \n",
    "    'cv__min_df': [3, 5],  #minimum number of docments something needs to be in in order to be included\n",
    "    \n",
    "    'cv__ngram_range': [(1,1), (1,2)],   #Accounts for context of up to two words i.e  'not good' vs 'not' or 'good'\n",
    "    \n",
    "    'lr__C': [.5, 1]  #penalty on coefficients increases as C decreases\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwama\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CountVectorizer, Logistic Regression train score is 0.93375\n",
      "The CountVectorizer, Logistic Regression test score is 0.9344569288389513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cv__max_features': 1500,\n",
       " 'cv__min_df': 3,\n",
       " 'cv__ngram_range': (1, 2),\n",
       " 'lr__C': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearching to find best combination of hyperparameters for CountVectorizer\n",
    "gs_cv_lr = GridSearchCV(pipe_cv_lr, param_grid=pipe_cv_lr_params, cv = 3)\n",
    "gs_cv_lr.fit(X_train, y_train)\n",
    "print(f'The CountVectorizer, Logistic Regression train score is {gs_cv_lr.best_score_}')\n",
    "print(f'The CountVectorizer, Logistic Regression test score is {gs_cv_lr.score(X_test, y_test)}')\n",
    "gs_cv_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Negative</th>\n",
       "      <th>Actual Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicted Negative</th>\n",
       "      <td>105</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted Positive</th>\n",
       "      <td>11</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Actual Negative  Actual Positive\n",
       "Predicted Negative              105               24\n",
       "Predicted Positive               11              394"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_true=y_test,   #Actuals\n",
    "                                            y_pred=gs_cv_lr.predict(X_test)),   #Generate Predictions\n",
    "             columns=['Actual Negative', 'Actual Positive'],\n",
    "             index=['Predicted Negative','Predicted Positive'])\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix lets us know how well our model is adapting to new data. The positive or majority class is Anime and the negative or minority class is KDrama. Because I stratified my data before I split it, the inequality in the number of true positive (*bottom right*) and true negative (*top left*) values lets you know that there is an imbalance in the amount of Anime and KDrama posts in my data. \n",
    "> * I have signigicantly more true positives and negatives than false positives (*bottom left*) and false negatives (*top right*), meaning that this model does a great job of correctly assigning new posts to the correct subreddit.\n",
    "* I have a good number of false negatives though, which lets me know that we are often assigning something to the KDrama class when it actually belongs to the Anime class.\n",
    "* The low number of false positives means that the model rarely assigns something to the KDrama class when it is really an anime. \n",
    "\n",
    "The last two cases are probably due to the fact that our data is imbalanced, causing our model to be less prepared to handle new KDrama data because it needs to learn a bit more about it before it can accurately be sorted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *TFIDF Vectorizer*\n",
    "\n",
    "Similar to Count Vectorizer, this NLP method converts my titles into strings. Unlike CountVectorizer, TFIDFVectorizer assigns a float score to each of the words in the title based on how often they appear in all of my documents.\n",
    "> * words that appear more often in one document but rarely in the rest of them will score higher (i.e names)\n",
    "* words that appear often in one document and show up in every document will score lower(i.e the)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates pipeline for TFIDFVectorizer and Logistic Regression\n",
    "pipe_tf_lr = Pipeline([('tf', TfidfVectorizer(stop_words = 'english', max_df = .9)),\n",
    "                ('lr', LogisticRegression(random_state = 42))])\n",
    "\n",
    "#Hyperparameters for GridSearch to use to find best possible tfVectorizer, Log Regression hyperparameters\n",
    "\n",
    "pipe_tf_lr_params = {\n",
    "    'tf__max_features': [3000, 3500, 4000],\n",
    "    'tf__min_df': [3, 5],\n",
    "    'tf__ngram_range': [(1,1), (1,2)],  \n",
    "    'lr__C': [.5, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwama\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TFIDFVectorizer, Logistic Regression train score is 0.9413439635535308\n",
      "The TFIDFVectorizer, Logistic Regression test score is 0.9538855678906917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lr__C': 1,\n",
       " 'tf__max_features': 3000,\n",
       " 'tf__min_df': 5,\n",
       " 'tf__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearching to find best combination of hyperparameters for tfVectorizer\n",
    "gs_tf_lr = GridSearchCV(pipe_tf_lr, param_grid=pipe_tf_lr_params, cv = 3)\n",
    "gs_tf_lr.fit(X_train, y_train)\n",
    "print(f'The TFIDFVectorizer, Logistic Regression train score is {gs_tf_lr.best_score_}')\n",
    "print(f'The TFIDFVectorizer, Logistic Regression test score is {gs_tf_lr.score(X_test, y_test)}')\n",
    "gs_tf_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_true=y_test,   #Actuals\n",
    "                                            y_pred=gs_tf_lr.predict(X_test)),   #Generate Predictions\n",
    "             columns=['Actual Negative', 'Actual Positive'],\n",
    "             index=['Predicted Negative','Predicted Positive'])\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It turns out that both Logistic Regression Models perform really strongly and adapt to new data really well.** The Count Vectorizer model is pulling ahead of the TFIDF model, but this could be changed by modifying the gridsearch hyperparameters. \n",
    "> Due to it's performance, I will be comparing the **Logistic Regression Count Vectorizer model (LRCV)** to the remaining models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Multinomial Naive Bayes\n",
    "\n",
    "This is a modeling technique that relies on Bayes Theorem to make classification. It also models with the assumption that all of our features are independent of one another, which is rarely met. Although that assumption is naive, this model performs amazingly well regardless of that fact. We will use the Multinomial Version because our column values are positive integers after CountVectorizering our data.\n",
    "\n",
    "> This model should give us a much better train and test score than the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates pipeline for TFIDFVectorizer and Gaussian Naive Bayes\n",
    "pipe_mnb = Pipeline([('cv', CountVectorizer(stop_words = 'english', max_df = .9)), \n",
    "                ('mnb', MultinomialNB())])\n",
    "\n",
    "#Hyperparameters for GridSearch to use to find best possible tfVec, Gaussian Naive Bayes hyperparameters\n",
    "\n",
    "pipe_mnb_params = {\n",
    "    'cv__max_features': [1500, 2000, 2500],\n",
    "    'cv__min_df': [3, 5],\n",
    "    'cv__ngram_range': [(1,1), (1,2)]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CountVectorizer, Multnomial Naive Bayes train score is 0.9496013667425968\n",
      "The CountVectorizer, Multinomial Naive Bayes test score is 0.9598633646456021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cv__max_features': 2500, 'cv__min_df': 3, 'cv__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearching to find best combination of hyperparameters for tfVectorizer\n",
    "gs_mnb = GridSearchCV(pipe_mnb, param_grid=pipe_mnb_params, cv = 3)\n",
    "gs_mnb.fit(X_train, y_train)\n",
    "print(f'The CountVectorizer, Multnomial Naive Bayes train score is {gs_mnb.best_score_}')\n",
    "print(f'The CountVectorizer, Multinomial Naive Bayes test score is {gs_mnb.score(X_test, y_test)}')\n",
    "gs_mnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_true=y_test,   #Actuals\n",
    "                                            y_pred=gs_mnb.predict(X_test)),   #Generate Predictions\n",
    "             columns=['Actual Negative', 'Actual Positive'],\n",
    "             index=['Predicted Negative','Predicted Positive'])\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Count Vectorized Multinomial Bayes model performed very strongly with an extremely high train score. It also sports high adaptability to new data with the test score being higher than the train score. However, our model is less accurate and a weaker performer when adapting to new data when compared to the LRCV model. This may be because the naive assumption that's trademark to the Naive Bayes model is causing some error. It may also be because of the hyperparameters currently set. \n",
    "\n",
    "*As we continue, one major factor in the performance of our models will be the hyperparameters that each model is tested on. The nature of grid searching is a guess and check, so a good estimate can lead to amazing results for one model, and poor results for another.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Gaussian Naive Bayes\n",
    "\n",
    "This is a modeling technique that relies on Bayes Theorem to make classification. It also models with the assumption that all of our features are independent of one another, which is rarely met. Although that assumption is naive, this model performs amazingly well regardless of that fact. We will use th Gaussian Version because the requirements for the alternative version are violated when we use the TFIDFVectorizor on our corpora. \n",
    "\n",
    "> This model should give us a much better train and test score than the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates pipeline for TFIDFVectorizer and Gaussian Naive Bayes\n",
    "pipe_gnb = Pipeline([('tf', TfidfVectorizer(stop_words = 'english', max_df = .9)),\n",
    "                ('to_dense', DenseTransformer()), \n",
    "                ('gnb', GaussianNB())])\n",
    "\n",
    "#Hyperparameters for GridSearch to use to find best possible tfVec, Gaussian Naive Bayes hyperparameters\n",
    "\n",
    "pipe_gnb_params = {\n",
    "    'tf__max_features': [3500, 4000, 4500],\n",
    "    'tf__min_df': [3, 5],\n",
    "    'tf__ngram_range': [(1,1), (1,2)]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TFIDFVectorizer, Gaussian Naive Bayes train score is 0.941628701594533\n",
      "The TFIDFVectorizer, Gaussian Naive Bayes test score is 0.9350982066609735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tf__max_features': 4000, 'tf__min_df': 5, 'tf__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearching to find best combination of hyperparameters for tfVectorizer\n",
    "gs_gnb = GridSearchCV(pipe_gnb, param_grid=pipe_gnb_params, cv = 3)\n",
    "gs_gnb.fit(X_train, y_train)\n",
    "print(f'The TFIDFVectorizer, Gaussian Naive Bayes train score is {gs_gnb.best_score_}')\n",
    "print(f'The TFIDFVectorizer, Gaussian Naive Bayes test score is {gs_gnb.score(X_test, y_test)}')\n",
    "gs_gnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_true=y_test,   #Actuals\n",
    "                                            y_pred=gs_gnb.predict(X_test)),   #Generate Predictions\n",
    "             columns=['Actual Negative', 'Actual Positive'],\n",
    "             index=['Predicted Negative','Predicted Positive'])\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Gaussian Naive Bayes Model did very well, but doesn't match up to the LRCV. Although our data is slightly overfit, we can see that this is still a very strong model that adapts to new data with high accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Decision Tree\n",
    "\n",
    "Decision Trees are very prone to overfitting, so we implemented some hyperparamters when fitting to alleviate that. Decision trees work by attempting to reduce a Gini score to 0. A gini score is basically how pure a group of sample data is. A model with a gini score of 0 has final samples that consist of the same thing (i.e. all red birds). Decision trees want to get 1 sample per final sample group, which is what leads to overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Count Vectorizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_dt_cv = Pipeline([('cv', CountVectorizer(stop_words = 'english', max_df = .9)),\n",
    "                ('dt', DecisionTreeClassifier(max_depth = 5, random_state = 42))])\n",
    "\n",
    "#max_depth is the number of levels/questions asked by the decision tree\n",
    "\n",
    "pipe_dt_cv_params = {\n",
    "    'cv__max_features': [1500, 2000, 2500],\n",
    "    'cv__min_df': [3, 5],\n",
    "    'cv__ngram_range': [(1,1), (1,2)],\n",
    "    'dt__min_samples_leaf': [3, 5],   #Minimum number of samples required before splitting a group/node\n",
    "    'dt__min_samples_split': [7, 10]   #Minimum number of samples required to be in a group/node\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CountVectorizer, Decision Tree train score is 0.9068906605922551\n",
      "The CountVectorizer, Decision Tree test score is 0.9222886421861657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cv__max_features': 2500,\n",
       " 'cv__min_df': 5,\n",
       " 'cv__ngram_range': (1, 1),\n",
       " 'dt__min_samples_leaf': 3,\n",
       " 'dt__min_samples_split': 7}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearching to find best combination of hyperparameters for cvVectorizer\n",
    "gs_dt_cv = GridSearchCV(pipe_dt_cv, param_grid=pipe_dt_cv_params, cv = 3)\n",
    "gs_dt_cv.fit(X_train, y_train)\n",
    "print(f'The CountVectorizer, Decision Tree train score is {gs_dt_cv.best_score_}')\n",
    "print(f'The CountVectorizer, Decision Tree test score is {gs_dt_cv.score(X_test, y_test)}')\n",
    "gs_dt_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_true=y_test,   #Actuals\n",
    "                                            y_pred=gs_dt_cv.predict(X_test)),   #Generate Predictions\n",
    "             columns=['Actual Negative', 'Actual Positive'],\n",
    "             index=['Predicted Negative','Predicted Positive'])\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *TFIDF Vectorizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_dt_tf = Pipeline([('tf', TfidfVectorizer(stop_words = 'english', max_df = .9)),\n",
    "                ('dt', DecisionTreeClassifier(max_depth = 5, random_state = 42))])\n",
    "\n",
    "#max_depth is the number of levels/questions asked by the decision tree\n",
    "\n",
    "pipe_dt_tf_params = {\n",
    "    'tf__max_features': [3500, 4000, 4500],\n",
    "    'tf__min_df': [3, 5],\n",
    "    'tf__ngram_range': [(1,1), (1,2)],\n",
    "    'dt__min_samples_leaf': [3, 5],   #Minimum number of samples required before splitting a group/node\n",
    "    'dt__min_samples_split': [7, 10]   #Minimum number of samples required to be in a group/node\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TFIDFVectorizer, Decision Tree train score is 0.9094533029612756\n",
      "The TFIDFVectorizer, Decision Tree test score is 0.9231426131511529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dt__min_samples_leaf': 3,\n",
       " 'dt__min_samples_split': 7,\n",
       " 'tf__max_features': 4500,\n",
       " 'tf__min_df': 5,\n",
       " 'tf__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearching to find best combination of hyperparameters for tfVectorizer\n",
    "gs_dt_tf = GridSearchCV(pipe_dt_tf, param_grid=pipe_dt_tf_params, cv = 3)\n",
    "gs_dt_tf.fit(X_train, y_train)\n",
    "print(f'The TFIDFVectorizer, Decision Tree train score is {gs_dt_tf.best_score_}')\n",
    "print(f'The TFIDFVectorizer, Decision Tree test score is {gs_dt_tf.score(X_test, y_test)}')\n",
    "gs_dt_tf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_true=y_test,   #Actuals\n",
    "                                            y_pred=gs_dt_tf.predict(X_test)),   #Generate Predictions\n",
    "             columns=['Actual Negative', 'Actual Positive'],\n",
    "             index=['Predicted Negative','Predicted Positive'])\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the parameters I have inserted to lower the error due to variance have worked considerably well. In fact, these decision tree models adapt extremely well to new data. This shows the strengths of grid searching and hyperparameter tuning. The right combination can make an ok model a great one, but it is still a far cry from the LRCV model. This may be due to the imbalance in my data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Random Forest \n",
    "\n",
    "Random Forests are aggregated decision trees that use a concept called bagging to make them very accurate. Bagging is basically the central limit theorem; we split a population (our data) into multiple samples and run decision tree models over each sample. Then we take the average of each model. This is an ensemble model, and these models typically have the highest accuracy scores for three reasons. \n",
    "> * First, ensemble models take the average of multiple models, which usually results in canceling out the error in each of the individuals models. \n",
    "* Second, taking the average scores of multiple models tends to result in reaching scores one model may not have been able to reach alone. This means we may be able to hone in on a global best. \n",
    "* Finally, one model will most likely be not perfect since they all have their shortcomings. Aggregating the results of multiple models could create a model that exceeds the limitations of all the components combined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Count Vectorizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf_cv = Pipeline([('cv', CountVectorizer(stop_words = 'english', max_df = .9)),\n",
    "                ('rf', RandomForestClassifier(max_depth = 5, random_state = 42))])\n",
    "\n",
    "pipe_rf_cv_params = {\n",
    "    'cv__max_features': [2500, 3500, 4000, 4500],\n",
    "    'cv__min_df': [3, 5],\n",
    "    'cv__ngram_range': [(1,1), (1,2)],\n",
    "    'rf__n_estimators': [50 ,75],\n",
    "    'rf__max_depth': [5, 6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CountVectorizer, Random Forest train score is 0.8345671981776766\n",
      "The CountVectorizer, Random Forest test score is 0.8317677198975235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cv__max_features': 2500,\n",
       " 'cv__min_df': 5,\n",
       " 'cv__ngram_range': (1, 2),\n",
       " 'rf__max_depth': 6,\n",
       " 'rf__n_estimators': 50}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearching to find best combination of hyperparameters for tfVectorizer\n",
    "gs_rf_cv = GridSearchCV(pipe_rf_cv, param_grid=pipe_rf_cv_params, cv = 3)\n",
    "gs_rf_cv.fit(X_train, y_train)\n",
    "print(f'The CountVectorizer, Random Forest train score is {gs_rf_cv.best_score_}')\n",
    "print(f'The CountVectorizer, Random Forest test score is {gs_rf_cv.score(X_test, y_test)}')\n",
    "gs_rf_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_true=y_test,   #Actuals\n",
    "                                            y_pred=gs_rf_cv.predict(X_test)),   #Generate Predictions\n",
    "             columns=['Actual Negative', 'Actual Positive'],\n",
    "             index=['Predicted Negative','Predicted Positive'])\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *TFIDF Vectorizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf_tf = Pipeline([('tf', TfidfVectorizer(stop_words = 'english', max_df = .9)),\n",
    "                ('rf', RandomForestClassifier(max_depth = 5, random_state = 42))])\n",
    "\n",
    "pipe_rf_tf_params = {\n",
    "    'tf__max_features': [2500, 3500, 4000, 4500],\n",
    "    'tf__min_df': [3, 5],\n",
    "    'tf__ngram_range': [(1,1), (1,2)],\n",
    "    'rf__n_estimators': [50 ,75],\n",
    "    'rf__max_depth': [5, 6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TFIDFVectorizer, Random Forest train score is 0.8354214123006833\n",
      "The TFIDFVectorizer, Random Forest test score is 0.8317677198975235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rf__max_depth': 6,\n",
       " 'rf__n_estimators': 50,\n",
       " 'tf__max_features': 2500,\n",
       " 'tf__min_df': 5,\n",
       " 'tf__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearching to find best combination of hyperparameters for tfVectorizer\n",
    "gs_rf_tf = GridSearchCV(pipe_rf_tf, param_grid=pipe_rf_tf_params, cv = 3)\n",
    "gs_rf_tf.fit(X_train, y_train)\n",
    "print(f'The TFIDFVectorizer, Random Forest train score is {gs_rf_tf.best_score_}')\n",
    "print(f'The TFIDFVectorizer, Random Forest test score is {gs_rf_tf.score(X_test, y_test)}')\n",
    "gs_rf_tf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = pd.DataFrame(confusion_matrix(y_true=y_test,   #Actuals\n",
    "                                            y_pred=gs_rt_tf.predict(X_test)),   #Generate Predictions\n",
    "             columns=['Actual Negative', 'Actual Positive'],\n",
    "             index=['Predicted Negative','Predicted Positive'])\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shockingly, the random forest model actually yielded the worst results. Ensemble models are typically the most accurate of all the models and adapts to new data remarkably well. Since this model is built using an aggregation of decision trees, it should have at least performed better than them. I'm going to attribute this error to the tradeoff with gridsearching. I could pass in all the available hyperparameters for random forests, but then it may take a whole day for my model to run. Grid searching is also a guess and check method, and I may have picked a poor range of possible hyperparameters. That is definitely something I would want to change in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model accuracy score is 0.7157804825966261.\n",
      "The CountVectorizer, Logistic Regression train score is 0.9689635535307517\n",
      "The TFIDFVectorizer, Logistic Regression train score is 0.9413439635535308\n",
      "The CountVectorizer, Multnomial Naive Bayes train score is 0.9496013667425968\n",
      "The TFIDFVectorizer, Gaussian Naive Bayes train score is 0.941628701594533\n",
      "The CountVectorizer, Decision Tree train score is 0.9068906605922551\n",
      "The TFIDFVectorizer, Decision Tree train score is 0.9094533029612756\n",
      "The CountVectorizer, Random Forest train score is 0.8345671981776766\n",
      "The TFIDFVectorizer, Random Forest train score is 0.8354214123006833\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score\n",
    "print(f'Baseline model accuracy score is {y.value_counts(normalize = True).max()}.')\n",
    "print(f'The CountVectorizer, Logistic Regression train score is {gs_cv_lr.best_score_}')\n",
    "print(f'The TFIDFVectorizer, Logistic Regression train score is {gs_tf_lr.best_score_}')\n",
    "print(f'The CountVectorizer, Multnomial Naive Bayes train score is {gs_mnb.best_score_}')\n",
    "print(f'The TFIDFVectorizer, Gaussian Naive Bayes train score is {gs_gnb.best_score_}')\n",
    "print(f'The CountVectorizer, Decision Tree train score is {gs_dt_cv.best_score_}')\n",
    "print(f'The TFIDFVectorizer, Decision Tree train score is {gs_dt_tf.best_score_}')\n",
    "print(f'The CountVectorizer, Random Forest train score is {gs_rf_cv.best_score_}')\n",
    "print(f'The TFIDFVectorizer, Random Forest train score is {gs_rf_tf.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Although all of our models performed better than the baseline, I would go with the CountVectorized Logisitic Regression model to sort between the Anime and KDrama subreddit. It sported the best train and test scores while not being overfit. In the future, I could definitely increase the accuracy of my models by tweaking the hyperparameters each model gridsearches over. Our worst performing models were decision trees and random forests, which is probably due to our dataset imbalance. Our strongest performing models were the Logisitic Regression models, which is without a doubt due to the fact that we have an easily binarized target variable. The Naive Bayes models also performed really strongly and would be solid alternatives to the Logistic Regression models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
